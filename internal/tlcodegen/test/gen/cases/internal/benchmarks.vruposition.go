// Copyright 2025 V Kontakte LLC
//
// This Source Code Form is subject to the terms of the Mozilla Public
// License, v. 2.0. If a copy of the MPL was not distributed with this
// file, You can obtain one at https://mozilla.org/MPL/2.0/.

// Code generated by vktl/cmd/tlgen2; DO NOT EDIT.
package internal

import (
	"github.com/vkcom/tl/pkg/basictl"
)

var _ = basictl.NatWrite

type BenchmarksVruposition struct {
	FieldsMask uint32
	// CommitBit (TrueType) // Conditional: item.FieldsMask.0
	// MetaBlock (TrueType) // Conditional: item.FieldsMask.1
	// SplitPayload (TrueType) // Conditional: item.FieldsMask.3
	// RotationBlock (TrueType) // Conditional: item.FieldsMask.5
	// CanonicalHash (TrueType) // Conditional: item.FieldsMask.15
	PayloadOffset int64
	BlockTimeNano int64
	Hash          BenchmarksVruhash
	FileOffset    int64
	SeqNumber     int64 // Conditional: item.FieldsMask.14
	tl2mask0      byte
}

func (BenchmarksVruposition) TLName() string { return "benchmarks.vruposition" }
func (BenchmarksVruposition) TLTag() uint32  { return 0x32792c04 }

func (item *BenchmarksVruposition) SetCommitBit(v bool) {
	if v {
		item.FieldsMask |= 1 << 0
	} else {
		item.FieldsMask &^= 1 << 0
	}
	if v {
		item.tl2mask0 |= 1
	} else {
		item.tl2mask0 &^= 1
	}
}
func (item *BenchmarksVruposition) IsSetCommitBit() bool { return item.tl2mask0&1 != 0 }

func (item *BenchmarksVruposition) SetMetaBlock(v bool) {
	if v {
		item.FieldsMask |= 1 << 1
	} else {
		item.FieldsMask &^= 1 << 1
	}
	if v {
		item.tl2mask0 |= 2
	} else {
		item.tl2mask0 &^= 2
	}
}
func (item *BenchmarksVruposition) IsSetMetaBlock() bool { return item.tl2mask0&2 != 0 }

func (item *BenchmarksVruposition) SetSplitPayload(v bool) {
	if v {
		item.FieldsMask |= 1 << 3
	} else {
		item.FieldsMask &^= 1 << 3
	}
	if v {
		item.tl2mask0 |= 4
	} else {
		item.tl2mask0 &^= 4
	}
}
func (item *BenchmarksVruposition) IsSetSplitPayload() bool { return item.tl2mask0&4 != 0 }

func (item *BenchmarksVruposition) SetRotationBlock(v bool) {
	if v {
		item.FieldsMask |= 1 << 5
	} else {
		item.FieldsMask &^= 1 << 5
	}
	if v {
		item.tl2mask0 |= 8
	} else {
		item.tl2mask0 &^= 8
	}
}
func (item *BenchmarksVruposition) IsSetRotationBlock() bool { return item.tl2mask0&8 != 0 }

func (item *BenchmarksVruposition) SetCanonicalHash(v bool) {
	if v {
		item.FieldsMask |= 1 << 15
	} else {
		item.FieldsMask &^= 1 << 15
	}
	if v {
		item.tl2mask0 |= 16
	} else {
		item.tl2mask0 &^= 16
	}
}
func (item *BenchmarksVruposition) IsSetCanonicalHash() bool { return item.tl2mask0&16 != 0 }

func (item *BenchmarksVruposition) SetSeqNumber(v int64) {
	item.SeqNumber = v
	item.FieldsMask |= 1 << 14
	item.tl2mask0 |= 32
}
func (item *BenchmarksVruposition) ClearSeqNumber() {
	item.SeqNumber = 0
	item.FieldsMask &^= 1 << 14
	item.tl2mask0 &^= 32
}
func (item *BenchmarksVruposition) IsSetSeqNumber() bool { return item.tl2mask0&32 != 0 }

func (item *BenchmarksVruposition) Reset() {
	item.FieldsMask = 0
	item.PayloadOffset = 0
	item.BlockTimeNano = 0
	item.Hash.Reset()
	item.FileOffset = 0
	item.SeqNumber = 0
	item.tl2mask0 = 0
}

func (item *BenchmarksVruposition) FillRandom(rg *basictl.RandGenerator) {
	item.tl2mask0 = 0
	item.FieldsMask = basictl.RandomFieldMask(rg, 0b1100000000101011)
	if item.FieldsMask&(1<<0) != 0 {
		item.tl2mask0 |= 1
	}
	if item.FieldsMask&(1<<1) != 0 {
		item.tl2mask0 |= 2
	}
	if item.FieldsMask&(1<<3) != 0 {
		item.tl2mask0 |= 4
	}
	if item.FieldsMask&(1<<5) != 0 {
		item.tl2mask0 |= 8
	}
	if item.FieldsMask&(1<<15) != 0 {
		item.tl2mask0 |= 16
	}
	item.PayloadOffset = basictl.RandomLong(rg)
	item.BlockTimeNano = basictl.RandomLong(rg)
	item.Hash.FillRandom(rg)
	item.FileOffset = basictl.RandomLong(rg)
	if item.FieldsMask&(1<<14) != 0 {
		item.tl2mask0 |= 32
		item.SeqNumber = basictl.RandomLong(rg)
	} else {
		item.SeqNumber = 0
	}
}

func (item BenchmarksVruposition) RepairMasksValue() BenchmarksVruposition {
	item.RepairMasks()
	return item
}
func (item *BenchmarksVruposition) RepairMasks() {
	item.tl2mask0 = 0
	if item.FieldsMask&(1<<0) != 0 {
		item.tl2mask0 |= 1
	}
	if item.FieldsMask&(1<<1) != 0 {
		item.tl2mask0 |= 2
	}
	if item.FieldsMask&(1<<3) != 0 {
		item.tl2mask0 |= 4
	}
	if item.FieldsMask&(1<<5) != 0 {
		item.tl2mask0 |= 8
	}
	if item.FieldsMask&(1<<15) != 0 {
		item.tl2mask0 |= 16
	}
	if item.FieldsMask&(1<<14) != 0 {
		item.tl2mask0 |= 32
	}
}

func (item *BenchmarksVruposition) Read(w []byte) (_ []byte, err error) {
	item.tl2mask0 = 0
	if w, err = basictl.NatRead(w, &item.FieldsMask); err != nil {
		return w, err
	}
	if item.FieldsMask&(1<<0) != 0 {
		item.tl2mask0 |= 1
	}
	if item.FieldsMask&(1<<1) != 0 {
		item.tl2mask0 |= 2
	}
	if item.FieldsMask&(1<<3) != 0 {
		item.tl2mask0 |= 4
	}
	if item.FieldsMask&(1<<5) != 0 {
		item.tl2mask0 |= 8
	}
	if item.FieldsMask&(1<<15) != 0 {
		item.tl2mask0 |= 16
	}
	if w, err = basictl.LongRead(w, &item.PayloadOffset); err != nil {
		return w, err
	}
	if w, err = basictl.LongRead(w, &item.BlockTimeNano); err != nil {
		return w, err
	}
	if w, err = item.Hash.Read(w); err != nil {
		return w, err
	}
	if w, err = basictl.LongRead(w, &item.FileOffset); err != nil {
		return w, err
	}
	if item.FieldsMask&(1<<14) != 0 {
		item.tl2mask0 |= 32
		if w, err = basictl.LongRead(w, &item.SeqNumber); err != nil {
			return w, err
		}
	} else {
		item.SeqNumber = 0
	}
	return w, nil
}

func (item *BenchmarksVruposition) WriteGeneral(w []byte) (_ []byte, err error) {
	return item.Write(w), nil
}

func (item *BenchmarksVruposition) Write(w []byte) []byte {
	w = basictl.NatWrite(w, item.FieldsMask)
	w = basictl.LongWrite(w, item.PayloadOffset)
	w = basictl.LongWrite(w, item.BlockTimeNano)
	w = item.Hash.Write(w)
	w = basictl.LongWrite(w, item.FileOffset)
	if item.FieldsMask&(1<<14) != 0 {
		w = basictl.LongWrite(w, item.SeqNumber)
	}
	return w
}

func (item *BenchmarksVruposition) ReadBoxed(w []byte) (_ []byte, err error) {
	if w, err = basictl.NatReadExactTag(w, 0x32792c04); err != nil {
		return w, err
	}
	return item.Read(w)
}

func (item *BenchmarksVruposition) WriteBoxedGeneral(w []byte) (_ []byte, err error) {
	return item.WriteBoxed(w), nil
}

func (item *BenchmarksVruposition) WriteBoxed(w []byte) []byte {
	w = basictl.NatWrite(w, 0x32792c04)
	return item.Write(w)
}

func (item BenchmarksVruposition) String() string {
	return string(item.WriteJSON(nil))
}

func (item *BenchmarksVruposition) ReadJSON(legacyTypeNames bool, in *basictl.JsonLexer) error {
	tctx := basictl.JSONReadContext{LegacyTypeNames: legacyTypeNames}
	return item.ReadJSONGeneral(&tctx, in)
}

func (item *BenchmarksVruposition) ReadJSONGeneral(tctx *basictl.JSONReadContext, in *basictl.JsonLexer) error {
	var propFieldsMaskPresented bool
	var trueTypeCommitBitPresented bool
	var trueTypeCommitBitValue bool
	var trueTypeMetaBlockPresented bool
	var trueTypeMetaBlockValue bool
	var trueTypeSplitPayloadPresented bool
	var trueTypeSplitPayloadValue bool
	var trueTypeRotationBlockPresented bool
	var trueTypeRotationBlockValue bool
	var trueTypeCanonicalHashPresented bool
	var trueTypeCanonicalHashValue bool
	var propPayloadOffsetPresented bool
	var propBlockTimeNanoPresented bool
	var propHashPresented bool
	var propFileOffsetPresented bool
	var propSeqNumberPresented bool

	if in != nil {
		in.Delim('{')
		if !in.Ok() {
			return in.Error()
		}
		for !in.IsDelim('}') {
			key := in.UnsafeFieldName(true)
			in.WantColon()
			switch key {
			case "fields_mask":
				if propFieldsMaskPresented {
					return ErrorInvalidJSONWithDuplicatingKeys("benchmarks.vruposition", "fields_mask")
				}
				if err := Json2ReadUint32(in, &item.FieldsMask); err != nil {
					return err
				}
				propFieldsMaskPresented = true
			case "commit_bit":
				if trueTypeCommitBitPresented {
					return ErrorInvalidJSONWithDuplicatingKeys("benchmarks.vruposition", "commit_bit")
				}
				if err := Json2ReadBool(in, &trueTypeCommitBitValue); err != nil {
					return err
				}
				trueTypeCommitBitPresented = true
			case "meta_block":
				if trueTypeMetaBlockPresented {
					return ErrorInvalidJSONWithDuplicatingKeys("benchmarks.vruposition", "meta_block")
				}
				if err := Json2ReadBool(in, &trueTypeMetaBlockValue); err != nil {
					return err
				}
				trueTypeMetaBlockPresented = true
			case "split_payload":
				if trueTypeSplitPayloadPresented {
					return ErrorInvalidJSONWithDuplicatingKeys("benchmarks.vruposition", "split_payload")
				}
				if err := Json2ReadBool(in, &trueTypeSplitPayloadValue); err != nil {
					return err
				}
				trueTypeSplitPayloadPresented = true
			case "rotation_block":
				if trueTypeRotationBlockPresented {
					return ErrorInvalidJSONWithDuplicatingKeys("benchmarks.vruposition", "rotation_block")
				}
				if err := Json2ReadBool(in, &trueTypeRotationBlockValue); err != nil {
					return err
				}
				trueTypeRotationBlockPresented = true
			case "canonical_hash":
				if trueTypeCanonicalHashPresented {
					return ErrorInvalidJSONWithDuplicatingKeys("benchmarks.vruposition", "canonical_hash")
				}
				if err := Json2ReadBool(in, &trueTypeCanonicalHashValue); err != nil {
					return err
				}
				trueTypeCanonicalHashPresented = true
			case "payload_offset":
				if propPayloadOffsetPresented {
					return ErrorInvalidJSONWithDuplicatingKeys("benchmarks.vruposition", "payload_offset")
				}
				if err := Json2ReadInt64(in, &item.PayloadOffset); err != nil {
					return err
				}
				propPayloadOffsetPresented = true
			case "block_time_nano":
				if propBlockTimeNanoPresented {
					return ErrorInvalidJSONWithDuplicatingKeys("benchmarks.vruposition", "block_time_nano")
				}
				if err := Json2ReadInt64(in, &item.BlockTimeNano); err != nil {
					return err
				}
				propBlockTimeNanoPresented = true
			case "hash":
				if propHashPresented {
					return ErrorInvalidJSONWithDuplicatingKeys("benchmarks.vruposition", "hash")
				}
				if err := item.Hash.ReadJSONGeneral(tctx, in); err != nil {
					return err
				}
				propHashPresented = true
			case "file_offset":
				if propFileOffsetPresented {
					return ErrorInvalidJSONWithDuplicatingKeys("benchmarks.vruposition", "file_offset")
				}
				if err := Json2ReadInt64(in, &item.FileOffset); err != nil {
					return err
				}
				propFileOffsetPresented = true
			case "seq_number":
				if propSeqNumberPresented {
					return ErrorInvalidJSONWithDuplicatingKeys("benchmarks.vruposition", "seq_number")
				}
				if err := Json2ReadInt64(in, &item.SeqNumber); err != nil {
					return err
				}
				propSeqNumberPresented = true
			default:
				return ErrorInvalidJSONExcessElement("benchmarks.vruposition", key)
			}
			in.WantComma()
		}
		in.Delim('}')
		if !in.Ok() {
			return in.Error()
		}
	}
	if !propFieldsMaskPresented {
		item.FieldsMask = 0
	}
	if !propPayloadOffsetPresented {
		item.PayloadOffset = 0
	}
	if !propBlockTimeNanoPresented {
		item.BlockTimeNano = 0
	}
	if !propHashPresented {
		item.Hash.Reset()
	}
	if !propFileOffsetPresented {
		item.FileOffset = 0
	}
	if !propSeqNumberPresented {
		item.SeqNumber = 0
	}
	if trueTypeCommitBitPresented {
		if trueTypeCommitBitValue {
			item.FieldsMask |= 1 << 0
		}
	}
	if trueTypeMetaBlockPresented {
		if trueTypeMetaBlockValue {
			item.FieldsMask |= 1 << 1
		}
	}
	if trueTypeSplitPayloadPresented {
		if trueTypeSplitPayloadValue {
			item.FieldsMask |= 1 << 3
		}
	}
	if trueTypeRotationBlockPresented {
		if trueTypeRotationBlockValue {
			item.FieldsMask |= 1 << 5
		}
	}
	if trueTypeCanonicalHashPresented {
		if trueTypeCanonicalHashValue {
			item.FieldsMask |= 1 << 15
		}
	}
	if propSeqNumberPresented {
		item.FieldsMask |= 1 << 14
	}
	// tries to set bit to zero if it is 1
	if trueTypeCommitBitPresented && !trueTypeCommitBitValue && (item.FieldsMask&(1<<0) != 0) {
		return ErrorInvalidJSON("benchmarks.vruposition", "fieldmask bit item.FieldsMask.0 is indefinite because of the contradictions in values")
	}
	// tries to set bit to zero if it is 1
	if trueTypeMetaBlockPresented && !trueTypeMetaBlockValue && (item.FieldsMask&(1<<1) != 0) {
		return ErrorInvalidJSON("benchmarks.vruposition", "fieldmask bit item.FieldsMask.1 is indefinite because of the contradictions in values")
	}
	// tries to set bit to zero if it is 1
	if trueTypeSplitPayloadPresented && !trueTypeSplitPayloadValue && (item.FieldsMask&(1<<3) != 0) {
		return ErrorInvalidJSON("benchmarks.vruposition", "fieldmask bit item.FieldsMask.3 is indefinite because of the contradictions in values")
	}
	// tries to set bit to zero if it is 1
	if trueTypeRotationBlockPresented && !trueTypeRotationBlockValue && (item.FieldsMask&(1<<5) != 0) {
		return ErrorInvalidJSON("benchmarks.vruposition", "fieldmask bit item.FieldsMask.5 is indefinite because of the contradictions in values")
	}
	// tries to set bit to zero if it is 1
	if trueTypeCanonicalHashPresented && !trueTypeCanonicalHashValue && (item.FieldsMask&(1<<15) != 0) {
		return ErrorInvalidJSON("benchmarks.vruposition", "fieldmask bit item.FieldsMask.15 is indefinite because of the contradictions in values")
	}
	if item.FieldsMask&(1<<0) != 0 {
		item.tl2mask0 |= 1
	}
	if item.FieldsMask&(1<<1) != 0 {
		item.tl2mask0 |= 2
	}
	if item.FieldsMask&(1<<3) != 0 {
		item.tl2mask0 |= 4
	}
	if item.FieldsMask&(1<<5) != 0 {
		item.tl2mask0 |= 8
	}
	if item.FieldsMask&(1<<15) != 0 {
		item.tl2mask0 |= 16
	}
	if item.FieldsMask&(1<<14) != 0 {
		item.tl2mask0 |= 32
	}
	return nil
}

// This method is general version of WriteJSON, use it instead!
func (item *BenchmarksVruposition) WriteJSONGeneral(tctx *basictl.JSONWriteContext, w []byte) (_ []byte, err error) {
	return item.WriteJSONOpt(tctx, w), nil
}

func (item *BenchmarksVruposition) WriteJSON(w []byte) []byte {
	tctx := basictl.JSONWriteContext{}
	return item.WriteJSONOpt(&tctx, w)
}
func (item *BenchmarksVruposition) WriteJSONOpt(tctx *basictl.JSONWriteContext, w []byte) []byte {
	w = append(w, '{')
	backupIndexFieldsMask := len(w)
	w = basictl.JSONAddCommaIfNeeded(w)
	w = append(w, `"fields_mask":`...)
	w = basictl.JSONWriteUint32(w, item.FieldsMask)
	if (item.FieldsMask != 0) == false {
		w = w[:backupIndexFieldsMask]
	}
	if item.FieldsMask&(1<<0) != 0 {
		w = basictl.JSONAddCommaIfNeeded(w)
		w = append(w, `"commit_bit":true`...)
	}
	if item.FieldsMask&(1<<1) != 0 {
		w = basictl.JSONAddCommaIfNeeded(w)
		w = append(w, `"meta_block":true`...)
	}
	if item.FieldsMask&(1<<3) != 0 {
		w = basictl.JSONAddCommaIfNeeded(w)
		w = append(w, `"split_payload":true`...)
	}
	if item.FieldsMask&(1<<5) != 0 {
		w = basictl.JSONAddCommaIfNeeded(w)
		w = append(w, `"rotation_block":true`...)
	}
	if item.FieldsMask&(1<<15) != 0 {
		w = basictl.JSONAddCommaIfNeeded(w)
		w = append(w, `"canonical_hash":true`...)
	}
	backupIndexPayloadOffset := len(w)
	w = basictl.JSONAddCommaIfNeeded(w)
	w = append(w, `"payload_offset":`...)
	w = basictl.JSONWriteInt64(w, item.PayloadOffset)
	if (item.PayloadOffset != 0) == false {
		w = w[:backupIndexPayloadOffset]
	}
	backupIndexBlockTimeNano := len(w)
	w = basictl.JSONAddCommaIfNeeded(w)
	w = append(w, `"block_time_nano":`...)
	w = basictl.JSONWriteInt64(w, item.BlockTimeNano)
	if (item.BlockTimeNano != 0) == false {
		w = w[:backupIndexBlockTimeNano]
	}
	w = basictl.JSONAddCommaIfNeeded(w)
	w = append(w, `"hash":`...)
	w = item.Hash.WriteJSONOpt(tctx, w)
	backupIndexFileOffset := len(w)
	w = basictl.JSONAddCommaIfNeeded(w)
	w = append(w, `"file_offset":`...)
	w = basictl.JSONWriteInt64(w, item.FileOffset)
	if (item.FileOffset != 0) == false {
		w = w[:backupIndexFileOffset]
	}
	if item.FieldsMask&(1<<14) != 0 {
		w = basictl.JSONAddCommaIfNeeded(w)
		w = append(w, `"seq_number":`...)
		w = basictl.JSONWriteInt64(w, item.SeqNumber)
	}
	return append(w, '}')
}

func (item *BenchmarksVruposition) MarshalJSON() ([]byte, error) {
	return item.WriteJSON(nil), nil
}

func (item *BenchmarksVruposition) UnmarshalJSON(b []byte) error {
	if err := item.ReadJSON(true, &basictl.JsonLexer{Data: b}); err != nil {
		return ErrorInvalidJSON("benchmarks.vruposition", err.Error())
	}
	return nil
}

func (item *BenchmarksVruposition) CalculateLayout(sizes []int, optimizeEmpty bool) ([]int, int) {
	sizes = append(sizes, 846801924)
	sizePosition := len(sizes)
	sizes = append(sizes, 0)

	currentSize := 1
	lastUsedByte := 0
	var sz int

	if item.FieldsMask != 0 {
		currentSize += 4
		lastUsedByte = currentSize
	}
	if item.tl2mask0&1 != 0 {
		lastUsedByte = currentSize
	}
	if item.tl2mask0&2 != 0 {
		lastUsedByte = currentSize
	}
	if item.tl2mask0&4 != 0 {
		lastUsedByte = currentSize
	}
	if item.tl2mask0&8 != 0 {
		lastUsedByte = currentSize
	}
	if item.tl2mask0&16 != 0 {
		lastUsedByte = currentSize
	}
	if item.PayloadOffset != 0 {
		currentSize += 8
		lastUsedByte = currentSize
	}
	currentSize++
	if item.BlockTimeNano != 0 {
		currentSize += 8
		lastUsedByte = currentSize
	}
	if sizes, sz = item.Hash.CalculateLayout(sizes, true); sz != 0 {
		currentSize += sz
		lastUsedByte = currentSize
	}
	if item.FileOffset != 0 {
		currentSize += 8
		lastUsedByte = currentSize
	}
	if item.tl2mask0&32 != 0 {
		currentSize += 8
		lastUsedByte = currentSize
	}

	if lastUsedByte < currentSize {
		currentSize = lastUsedByte
	}
	sizes[sizePosition] = currentSize
	if currentSize == 0 {
		sizes = sizes[:sizePosition+1]
	}
	if !optimizeEmpty || currentSize != 0 {
		currentSize += basictl.TL2CalculateSize(currentSize)
	}
	Unused(sz)
	return sizes, currentSize
}

func (item *BenchmarksVruposition) InternalWriteTL2(w []byte, sizes []int, optimizeEmpty bool) ([]byte, []int, int) {
	if sizes[0] != 846801924 {
		panic("tl2: tag mismatch between calculate and write")
	}
	currentSize := sizes[1]
	sizes = sizes[2:]
	if optimizeEmpty && currentSize == 0 {
		return w, sizes, 0
	}
	w = basictl.TL2WriteSize(w, currentSize)
	if currentSize == 0 {
		return w, sizes, 1
	}
	oldLen := len(w)
	var sz int
	var currentBlock byte
	currentBlockPosition := len(w)
	w = append(w, 0)
	if item.FieldsMask != 0 {
		w = basictl.NatWrite(w, item.FieldsMask)
		currentBlock |= 2
	}
	if item.tl2mask0&1 != 0 {
		currentBlock |= 4
	}
	if item.tl2mask0&2 != 0 {
		currentBlock |= 8
	}
	if item.tl2mask0&4 != 0 {
		currentBlock |= 16
	}
	if item.tl2mask0&8 != 0 {
		currentBlock |= 32
	}
	if item.tl2mask0&16 != 0 {
		currentBlock |= 64
	}
	if item.PayloadOffset != 0 {
		w = basictl.LongWrite(w, item.PayloadOffset)
		currentBlock |= 128
	}
	if currentBlockPosition < len(w) {
		w[currentBlockPosition] = currentBlock
	}
	currentBlock = 0
	// start the next block
	currentBlockPosition = len(w)
	if len(w)-oldLen < currentSize {
		w = append(w, 0)
	}
	if item.BlockTimeNano != 0 {
		w = basictl.LongWrite(w, item.BlockTimeNano)
		currentBlock |= 1
	}
	if w, sizes, sz = item.Hash.InternalWriteTL2(w, sizes, true); sz != 0 {
		currentBlock |= 2
	}
	if item.FileOffset != 0 {
		w = basictl.LongWrite(w, item.FileOffset)
		currentBlock |= 4
	}
	if item.tl2mask0&32 != 0 {
		w = basictl.LongWrite(w, item.SeqNumber)
		currentBlock |= 8
	}
	if currentBlockPosition < len(w) {
		w[currentBlockPosition] = currentBlock
	}
	if len(w)-oldLen != currentSize {
		panic("tl2: mismatch between calculate and write")
	}
	Unused(sz)
	return w, sizes, 1
}

func (item *BenchmarksVruposition) WriteTL2(w []byte, ctx *basictl.TL2WriteContext) []byte {
	var sizes, sizes2 []int
	if ctx != nil {
		sizes = ctx.SizeBuffer[:0]
	}
	sizes, _ = item.CalculateLayout(sizes, false)
	w, sizes2, _ = item.InternalWriteTL2(w, sizes, false)
	if len(sizes2) != 0 {
		panic("tl2: internal write did not consume all size data")
	}
	if ctx != nil {
		ctx.SizeBuffer = sizes
	}
	return w
}

func (item *BenchmarksVruposition) InternalReadTL2(r []byte) (_ []byte, err error) {
	currentSize := 0
	if r, currentSize, err = basictl.TL2ParseSize(r); err != nil {
		return r, err
	}
	if currentSize == 0 {
		item.Reset()
		return r, nil
	}
	if len(r) < currentSize {
		return r, basictl.TL2Error("not enough data: expected %d, got %d", currentSize, len(r))
	}

	currentR := r[:currentSize]
	r = r[currentSize:]

	var block byte
	if currentR, err = basictl.ByteReadTL2(currentR, &block); err != nil {
		return currentR, err
	}
	// read No of constructor
	if block&1 != 0 {
		var index int
		if currentR, index, err = basictl.TL2ParseSize(currentR); err != nil {
			return currentR, err
		}
		if index != 0 {
			return r, ErrorInvalidUnionIndex("benchmarks.vruposition", index)
		}
	}
	item.tl2mask0 = 0
	if block&2 != 0 {
		if currentR, err = basictl.NatRead(currentR, &item.FieldsMask); err != nil {
			return currentR, err
		}
	} else {
		item.FieldsMask = 0
	}
	if block&4 != 0 {
		item.tl2mask0 |= 1
	}
	if block&8 != 0 {
		item.tl2mask0 |= 2
	}
	if block&16 != 0 {
		item.tl2mask0 |= 4
	}
	if block&32 != 0 {
		item.tl2mask0 |= 8
	}
	if block&64 != 0 {
		item.tl2mask0 |= 16
	}
	if block&128 != 0 {
		if currentR, err = basictl.LongRead(currentR, &item.PayloadOffset); err != nil {
			return currentR, err
		}
	} else {
		item.PayloadOffset = 0
	}
	// start the next block
	if len(currentR) > 0 {
		if currentR, err = basictl.ByteReadTL2(currentR, &block); err != nil {
			return currentR, err
		}
	} else {
		block = 0
	}
	if block&1 != 0 {
		if currentR, err = basictl.LongRead(currentR, &item.BlockTimeNano); err != nil {
			return currentR, err
		}
	} else {
		item.BlockTimeNano = 0
	}
	if block&2 != 0 {
		if currentR, err = item.Hash.InternalReadTL2(currentR); err != nil {
			return currentR, err
		}
	} else {
		item.Hash.Reset()
	}
	if block&4 != 0 {
		if currentR, err = basictl.LongRead(currentR, &item.FileOffset); err != nil {
			return currentR, err
		}
	} else {
		item.FileOffset = 0
	}
	if block&8 != 0 {
		item.tl2mask0 |= 32
		if currentR, err = basictl.LongRead(currentR, &item.SeqNumber); err != nil {
			return currentR, err
		}
	} else {
		item.SeqNumber = 0
	}
	Unused(currentR)
	return r, nil
}

func (item *BenchmarksVruposition) ReadTL2(r []byte, ctx *basictl.TL2ReadContext) (_ []byte, err error) {
	return item.InternalReadTL2(r)
}

func BuiltinTupleBenchmarksVrupositionFillRandom(rg *basictl.RandGenerator, vec *[]BenchmarksVruposition, nat_n uint32) {
	rg.IncreaseDepth()
	*vec = make([]BenchmarksVruposition, nat_n)
	for i := range *vec {
		(*vec)[i].FillRandom(rg)
	}
	rg.DecreaseDepth()
}
func BuiltinTupleBenchmarksVrupositionRepairMasks(vec *[]BenchmarksVruposition, nat_n uint32) {
	for i := range *vec {
		(*vec)[i].RepairMasks()
	}
}

func BuiltinTupleBenchmarksVrupositionRead(w []byte, vec *[]BenchmarksVruposition, nat_n uint32) (_ []byte, err error) {
	if uint32(cap(*vec)) < nat_n {
		*vec = make([]BenchmarksVruposition, nat_n)
	} else {
		*vec = (*vec)[:nat_n]
	}
	for i := range *vec {
		if w, err = (*vec)[i].Read(w); err != nil {
			return w, err
		}
	}
	return w, nil
}

func BuiltinTupleBenchmarksVrupositionWrite(w []byte, vec []BenchmarksVruposition, nat_n uint32) (_ []byte, err error) {
	if uint32(len(vec)) != nat_n {
		return w, ErrorWrongSequenceLength("[]BenchmarksVruposition", len(vec), nat_n)
	}
	for _, elem := range vec {
		w = elem.Write(w)
	}
	return w, nil
}

func BuiltinTupleBenchmarksVrupositionCalculateLayout(sizes []int, optimizeEmpty bool, vec *[]BenchmarksVruposition) ([]int, int) {
	if len(*vec) == 0 {
		if optimizeEmpty {
			return sizes, 0
		}
		return sizes, 1
	}
	sizePosition := len(sizes)
	sizes = append(sizes, 0)

	currentSize := 0
	var sz int

	currentSize += basictl.TL2CalculateSize(len(*vec))
	for i := 0; i < len(*vec); i++ {
		sizes, sz = (*vec)[i].CalculateLayout(sizes, false)
		currentSize += sz
	}
	sizes[sizePosition] = currentSize
	currentSize += basictl.TL2CalculateSize(currentSize)
	Unused(sz)
	return sizes, currentSize
}

func BuiltinTupleBenchmarksVrupositionInternalWriteTL2(w []byte, sizes []int, optimizeEmpty bool, vec *[]BenchmarksVruposition) ([]byte, []int, int) {
	if len(*vec) == 0 {
		if optimizeEmpty {
			return w, sizes, 0
		}
		w = append(w, 0)
		return w, sizes, 1
	}
	currentSize := sizes[0]
	sizes = sizes[1:]
	w = basictl.TL2WriteSize(w, currentSize)
	if currentSize == 0 {
		return w, sizes, 1
	}
	oldLen := len(w)
	w = basictl.TL2WriteSize(w, len(*vec))

	var sz int
	for i := 0; i < len(*vec); i++ {
		w, sizes, _ = (*vec)[i].InternalWriteTL2(w, sizes, false)
	}
	Unused(sz)
	if len(w)-oldLen != currentSize {
		panic("tl2: mismatch between calculate and write")
	}
	return w, sizes, currentSize
}

func BuiltinTupleBenchmarksVrupositionInternalReadTL2(r []byte, vec *[]BenchmarksVruposition) (_ []byte, err error) {
	currentSize := 0
	if r, currentSize, err = basictl.TL2ParseSize(r); err != nil {
		return r, err
	}
	if len(r) < currentSize {
		return r, basictl.TL2Error("not enough data: expected %d, got %d", currentSize, len(r))
	}

	currentR := r[:currentSize]
	r = r[currentSize:]

	elementCount := 0
	if currentSize != 0 {
		if currentR, elementCount, err = basictl.TL2ParseSize(currentR); err != nil {
			return r, err
		}
		if elementCount > len(currentR) {
			return r, basictl.TL2ElementCountError(elementCount, currentR)
		}
	}

	if cap(*vec) < elementCount {
		*vec = make([]BenchmarksVruposition, elementCount)
	}
	*vec = (*vec)[:elementCount]
	for i := 0; i < elementCount; i++ {
		if currentR, err = (*vec)[i].InternalReadTL2(currentR); err != nil {
			return currentR, err
		}
	}
	return r, nil
}
func BuiltinTupleBenchmarksVrupositionReadJSONGeneral(tctx *basictl.JSONReadContext, in *basictl.JsonLexer, vec *[]BenchmarksVruposition, nat_n uint32) error {
	isTL2 := tctx != nil && tctx.IsTL2
	if isTL2 {
		nat_n = uint32(len(*vec))
	}
	if uint32(cap(*vec)) < nat_n {
		*vec = make([]BenchmarksVruposition, nat_n)
	} else {
		*vec = (*vec)[:nat_n]
	}
	index := 0
	if in != nil {
		in.Delim('[')
		if !in.Ok() {
			return ErrorInvalidJSON("[]BenchmarksVruposition", "expected json array")
		}
		for ; !in.IsDelim(']'); index++ {
			if nat_n <= uint32(index) {
				if isTL2 {
					var newValue BenchmarksVruposition
					*vec = append(*vec, newValue)
					*vec = (*vec)[:cap(*vec)]
					nat_n = uint32(len(*vec))
				} else {
					return ErrorInvalidJSON("[]BenchmarksVruposition", "array is longer than expected")
				}
			}
			if err := (*vec)[index].ReadJSONGeneral(tctx, in); err != nil {
				return err
			}
			in.WantComma()
		}
		in.Delim(']')
		if !in.Ok() {
			return ErrorInvalidJSON("[]BenchmarksVruposition", "expected json array's end")
		}
	}
	if isTL2 {
		*vec = (*vec)[:index]
	} else {
		if uint32(index) != nat_n {
			return ErrorWrongSequenceLength("[]BenchmarksVruposition", index, nat_n)
		}
	}
	return nil
}

func BuiltinTupleBenchmarksVrupositionWriteJSON(w []byte, vec []BenchmarksVruposition, nat_n uint32) (_ []byte, err error) {
	tctx := basictl.JSONWriteContext{}
	return BuiltinTupleBenchmarksVrupositionWriteJSONOpt(&tctx, w, vec, nat_n)
}
func BuiltinTupleBenchmarksVrupositionWriteJSONOpt(tctx *basictl.JSONWriteContext, w []byte, vec []BenchmarksVruposition, nat_n uint32) (_ []byte, err error) {
	if tctx != nil && tctx.IsTL2 {
		nat_n = uint32(len(vec))
	}
	if uint32(len(vec)) != nat_n {
		return w, ErrorWrongSequenceLength("[]BenchmarksVruposition", len(vec), nat_n)
	}
	w = append(w, '[')
	for _, elem := range vec {
		w = basictl.JSONAddCommaIfNeeded(w)
		w = elem.WriteJSONOpt(tctx, w)
	}
	return append(w, ']'), nil
}

func BuiltinVectorBenchmarksVrupositionFillRandom(rg *basictl.RandGenerator, vec *[]BenchmarksVruposition) {
	rg.IncreaseDepth()
	l := basictl.RandomSize(rg)
	*vec = make([]BenchmarksVruposition, l)
	for i := range *vec {
		(*vec)[i].FillRandom(rg)
	}
	rg.DecreaseDepth()
}
func BuiltinVectorBenchmarksVrupositionRepairMasks(vec *[]BenchmarksVruposition) {
	for i := range *vec {
		(*vec)[i].RepairMasks()
	}
}
func BuiltinVectorBenchmarksVrupositionRead(w []byte, vec *[]BenchmarksVruposition) (_ []byte, err error) {
	var l uint32
	if w, err = basictl.NatRead(w, &l); err != nil {
		return w, err
	}
	if uint32(cap(*vec)) < l {
		*vec = make([]BenchmarksVruposition, l)
	} else {
		*vec = (*vec)[:l]
	}
	for i := range *vec {
		if w, err = (*vec)[i].Read(w); err != nil {
			return w, err
		}
	}
	return w, nil
}

func BuiltinVectorBenchmarksVrupositionWrite(w []byte, vec []BenchmarksVruposition) []byte {
	w = basictl.NatWrite(w, uint32(len(vec)))
	for _, elem := range vec {
		w = elem.Write(w)
	}
	return w
}

func BuiltinVectorBenchmarksVrupositionCalculateLayout(sizes []int, optimizeEmpty bool, vec *[]BenchmarksVruposition) ([]int, int) {
	if len(*vec) == 0 {
		if optimizeEmpty {
			return sizes, 0
		}
		return sizes, 1
	}
	sizePosition := len(sizes)
	sizes = append(sizes, 0)

	currentSize := 0
	var sz int

	currentSize += basictl.TL2CalculateSize(len(*vec))
	for i := 0; i < len(*vec); i++ {
		sizes, sz = (*vec)[i].CalculateLayout(sizes, false)
		currentSize += sz
	}
	sizes[sizePosition] = currentSize
	currentSize += basictl.TL2CalculateSize(currentSize)
	Unused(sz)
	return sizes, currentSize
}

func BuiltinVectorBenchmarksVrupositionInternalWriteTL2(w []byte, sizes []int, optimizeEmpty bool, vec *[]BenchmarksVruposition) ([]byte, []int, int) {
	if len(*vec) == 0 {
		if optimizeEmpty {
			return w, sizes, 0
		}
		w = append(w, 0)
		return w, sizes, 1
	}
	currentSize := sizes[0]
	sizes = sizes[1:]
	w = basictl.TL2WriteSize(w, currentSize)
	if currentSize == 0 {
		return w, sizes, 1
	}
	oldLen := len(w)
	w = basictl.TL2WriteSize(w, len(*vec))

	var sz int
	for i := 0; i < len(*vec); i++ {
		w, sizes, _ = (*vec)[i].InternalWriteTL2(w, sizes, false)
	}
	Unused(sz)
	if len(w)-oldLen != currentSize {
		panic("tl2: mismatch between calculate and write")
	}
	return w, sizes, currentSize
}

func BuiltinVectorBenchmarksVrupositionInternalReadTL2(r []byte, vec *[]BenchmarksVruposition) (_ []byte, err error) {
	currentSize := 0
	if r, currentSize, err = basictl.TL2ParseSize(r); err != nil {
		return r, err
	}
	if len(r) < currentSize {
		return r, basictl.TL2Error("not enough data: expected %d, got %d", currentSize, len(r))
	}

	currentR := r[:currentSize]
	r = r[currentSize:]

	elementCount := 0
	if currentSize != 0 {
		if currentR, elementCount, err = basictl.TL2ParseSize(currentR); err != nil {
			return r, err
		}
		if elementCount > len(currentR) {
			return r, basictl.TL2ElementCountError(elementCount, currentR)
		}
	}

	if cap(*vec) < elementCount {
		*vec = make([]BenchmarksVruposition, elementCount)
	}
	*vec = (*vec)[:elementCount]
	for i := 0; i < elementCount; i++ {
		if currentR, err = (*vec)[i].InternalReadTL2(currentR); err != nil {
			return currentR, err
		}
	}
	return r, nil
}

func BuiltinVectorBenchmarksVrupositionReadJSONGeneral(tctx *basictl.JSONReadContext, in *basictl.JsonLexer, vec *[]BenchmarksVruposition) error {
	*vec = (*vec)[:cap(*vec)]
	index := 0
	if in != nil {
		in.Delim('[')
		if !in.Ok() {
			return ErrorInvalidJSON("[]BenchmarksVruposition", "expected json array")
		}
		for ; !in.IsDelim(']'); index++ {
			if len(*vec) <= index {
				var newValue BenchmarksVruposition
				*vec = append(*vec, newValue)
				*vec = (*vec)[:cap(*vec)]
			}
			if err := (*vec)[index].ReadJSONGeneral(tctx, in); err != nil {
				return err
			}
			in.WantComma()
		}
		in.Delim(']')
		if !in.Ok() {
			return ErrorInvalidJSON("[]BenchmarksVruposition", "expected json array's end")
		}
	}
	*vec = (*vec)[:index]
	return nil
}

func BuiltinVectorBenchmarksVrupositionWriteJSON(w []byte, vec []BenchmarksVruposition) []byte {
	tctx := basictl.JSONWriteContext{}
	return BuiltinVectorBenchmarksVrupositionWriteJSONOpt(&tctx, w, vec)
}
func BuiltinVectorBenchmarksVrupositionWriteJSONOpt(tctx *basictl.JSONWriteContext, w []byte, vec []BenchmarksVruposition) []byte {
	w = append(w, '[')
	for _, elem := range vec {
		w = basictl.JSONAddCommaIfNeeded(w)
		w = elem.WriteJSONOpt(tctx, w)
	}
	return append(w, ']')
}
